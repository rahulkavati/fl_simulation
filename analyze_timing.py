#!/usr/bin/env python3
"""
Federated Learning Timing Analysis
Comprehensive analysis of aggregation time, update time, and overall pipeline performance

Usage: python analyze_timing.py [--aggregation] [--global-update] [--pipeline] [--compare]
"""

import argparse
import os
import sys
import json
import time
import numpy as np
import pandas as pd
from pathlib import Path

def print_header(title: str):
    """Print a formatted header"""
    print(f"\n{'='*70}")
    print(f"‚è±Ô∏è  {title}")
    print(f"{'='*70}")

def print_section(title: str):
    """Print a formatted section header"""
    print(f"\nüîç {title}")
    print("-" * 50)

def print_success(message: str):
    """Print a success message"""
    print(f"‚úÖ {message}")

def print_error(message: str):
    """Print an error message"""
    print(f"‚ùå {message}")

def print_info(message: str):
    """Print an info message"""
    print(f"‚ÑπÔ∏è  {message}")

def print_metric(name: str, value: float, unit: str = "s", description: str = ""):
    """Print a metric with description"""
    if unit == "ms":
        print(f"  {name:20}: {value:.2f} {unit}")
    else:
        print(f"  {name:20}: {value:.4f} {unit}")
    if description:
        print(f"                      {description}")

def explain_timing_importance():
    """Explain why timing metrics are important in federated learning"""
    print_header("Why Aggregation Time Matters in Federated Learning")
    
    print_section("Performance Metrics")
    print("‚ö° Aggregation Time: Time to combine encrypted client updates")
    print("üîÑ Update Time: Time to apply aggregated updates to global model")
    print("üìä Total Pipeline Time: End-to-end federated learning round time")
    print("üîê Encryption Overhead: Additional time due to privacy protection")
    
    print_section("Why These Metrics Matter")
    print("üè• Real-world Deployment:")
    print("  ‚Ä¢ Faster aggregation = better user experience")
    print("  ‚Ä¢ Scalability for many clients (hospitals, devices)")
    print("  ‚Ä¢ Resource planning for server infrastructure")
    
    print("üîê Security vs Performance Trade-off:")
    print("  ‚Ä¢ Encryption adds computational overhead")
    print("  ‚Ä¢ Need to balance privacy protection with performance")
    print("  ‚Ä¢ Critical for production FL systems")
    
    print("üìà System Optimization:")
    print("  ‚Ä¢ Identify bottlenecks in the pipeline")
    print("  ‚Ä¢ Optimize encryption/decryption operations")
    print("  ‚Ä¢ Plan for scaling to more clients")
    
    print_section("Typical Timing Expectations")
    print("üü¢ Good Performance:")
    print("  ‚Ä¢ Aggregation: < 1 second for 5-10 clients")
    print("  ‚Ä¢ Global Update: < 0.5 seconds")
    print("  ‚Ä¢ Total Round: < 5 seconds")
    
    print("üü° Acceptable Performance:")
    print("  ‚Ä¢ Aggregation: 1-5 seconds for 10-50 clients")
    print("  ‚Ä¢ Global Update: 0.5-2 seconds")
    print("  ‚Ä¢ Total Round: 5-15 seconds")
    
    print("üî¥ Poor Performance:")
    print("  ‚Ä¢ Aggregation: > 5 seconds")
    print("  ‚Ä¢ Global Update: > 2 seconds")
    print("  ‚Ä¢ Total Round: > 15 seconds")

def analyze_aggregation_timing():
    """Analyze aggregation timing from smart switch"""
    print_header("Smart Switch Aggregation Timing Analysis")
    
    timing_dir = "Sriven/outbox"
    if not os.path.exists(timing_dir):
        print_error("No aggregation timing data found")
        return None
    
    # Find timing files
    timing_files = [f for f in os.listdir(timing_dir) if f.startswith('timing_round_') and f.endswith('.json')]
    
    if not timing_files:
        print_error("No timing files found in aggregation output")
        return None
    
    print_success(f"Found {len(timing_files)} timing files")
    
    # Load and analyze timing data
    timing_data = []
    for timing_file in sorted(timing_files):
        file_path = os.path.join(timing_dir, timing_file)
        with open(file_path, 'r') as f:
            data = json.load(f)
            timing_data.append(data)
    
    print_section("Aggregation Timing Results")
    
    # Calculate statistics
    agg_times = [d['aggregation_time_seconds'] for d in timing_data]
    total_times = [d['total_time_seconds'] for d in timing_data]
    num_clients = [d['num_clients'] for d in timing_data]
    
    print_metric("Average Aggregation Time", np.mean(agg_times), "s", "Time to combine encrypted updates")
    print_metric("Min Aggregation Time", np.min(agg_times), "s", "Fastest aggregation")
    print_metric("Max Aggregation Time", np.max(agg_times), "s", "Slowest aggregation")
    print_metric("Average Total Time", np.mean(total_times), "s", "Including I/O and validation")
    print_metric("Average Clients", np.mean(num_clients), "", "Number of clients per round")
    
    print_section("Per-Round Breakdown")
    for i, data in enumerate(timing_data):
        print(f"  Round {data['round_id']}:")
        print(f"    Clients: {data['num_clients']}")
        print(f"    Aggregation: {data['aggregation_time_seconds']:.4f}s")
        print(f"    Total: {data['total_time_seconds']:.4f}s")
    
    # Performance analysis
    print_section("Performance Analysis")
    avg_agg_time = np.mean(agg_times)
    avg_clients = np.mean(num_clients)
    
    if avg_agg_time < 1.0:
        print_success("üü¢ Excellent aggregation performance (< 1s)")
    elif avg_agg_time < 5.0:
        print_info("üü° Good aggregation performance (1-5s)")
    else:
        print_error("üî¥ Poor aggregation performance (> 5s)")
    
    # Scalability analysis
    time_per_client = avg_agg_time / avg_clients if avg_clients > 0 else 0
    print_metric("Time per Client", time_per_client, "s", "Scalability metric")
    
    if time_per_client < 0.2:
        print_success("üü¢ Excellent scalability (< 0.2s per client)")
    elif time_per_client < 1.0:
        print_info("üü° Good scalability (0.2-1s per client)")
    else:
        print_error("üî¥ Poor scalability (> 1s per client)")
    
    return timing_data

def analyze_global_update_timing():
    """Analyze global model update timing"""
    print_header("Global Model Update Timing Analysis")
    
    timing_dir = "updates/global_model"
    if not os.path.exists(timing_dir):
        print_error("No global update timing data found")
        return None
    
    # Find timing files
    timing_files = [f for f in os.listdir(timing_dir) if f.startswith('timing_round_') and f.endswith('.json')]
    
    if not timing_files:
        print_error("No global update timing files found")
        return None
    
    print_success(f"Found {len(timing_files)} global update timing files")
    
    # Load and analyze timing data
    timing_data = []
    for timing_file in sorted(timing_files):
        file_path = os.path.join(timing_dir, timing_file)
        with open(file_path, 'r') as f:
            data = json.load(f)
            timing_data.append(data)
    
    print_section("Global Update Timing Results")
    
    # Calculate statistics
    update_times = [d['update_time_seconds'] for d in timing_data]
    eval_times = [d.get('evaluation_time_seconds', 0) for d in timing_data]
    total_times = [d['total_time_seconds'] for d in timing_data]
    
    print_metric("Average Update Time", np.mean(update_times), "s", "Time to apply encrypted update")
    print_metric("Min Update Time", np.min(update_times), "s", "Fastest update")
    print_metric("Max Update Time", np.max(update_times), "s", "Slowest update")
    
    if any(eval_times):
        print_metric("Average Evaluation Time", np.mean(eval_times), "s", "Time for model evaluation")
        print_metric("Min Evaluation Time", np.min(eval_times), "s", "Fastest evaluation")
        print_metric("Max Evaluation Time", np.max(eval_times), "s", "Slowest evaluation")
    
    print_metric("Average Total Time", np.mean(total_times), "s", "Including evaluation")
    
    print_section("Per-Round Breakdown")
    for i, data in enumerate(timing_data):
        print(f"  Round {data['round_id']}:")
        print(f"    Update: {data['update_time_seconds']:.4f}s")
        if 'evaluation_time_seconds' in data:
            print(f"    Evaluation: {data['evaluation_time_seconds']:.4f}s")
        print(f"    Total: {data['total_time_seconds']:.4f}s")
    
    # Performance analysis
    print_section("Performance Analysis")
    avg_update_time = np.mean(update_times)
    
    if avg_update_time < 0.5:
        print_success("üü¢ Excellent update performance (< 0.5s)")
    elif avg_update_time < 2.0:
        print_info("üü° Good update performance (0.5-2s)")
    else:
        print_error("üî¥ Poor update performance (> 2s)")
    
    return timing_data

def analyze_pipeline_timing():
    """Analyze overall pipeline timing"""
    print_header("Complete Pipeline Timing Analysis")
    
    # Load aggregation timing
    agg_timing = analyze_aggregation_timing()
    if agg_timing is None:
        print_error("Cannot analyze pipeline without aggregation timing")
        return None
    
    # Load global update timing
    update_timing = analyze_global_update_timing()
    if update_timing is None:
        print_error("Cannot analyze pipeline without global update timing")
        return None
    
    print_section("Pipeline Performance Summary")
    
    # Calculate pipeline metrics
    agg_times = [d['aggregation_time_seconds'] for d in agg_timing]
    update_times = [d['update_time_seconds'] for d in update_timing]
    
    avg_agg_time = np.mean(agg_times)
    avg_update_time = np.mean(update_times)
    avg_pipeline_time = avg_agg_time + avg_update_time
    
    print_metric("Average Aggregation Time", avg_agg_time, "s", "Smart switch processing")
    print_metric("Average Update Time", avg_update_time, "s", "Global model update")
    print_metric("Average Pipeline Time", avg_pipeline_time, "s", "Complete FL round")
    
    # Performance assessment
    print_section("Overall Performance Assessment")
    
    if avg_pipeline_time < 2.0:
        print_success("üü¢ Excellent pipeline performance (< 2s total)")
    elif avg_pipeline_time < 10.0:
        print_info("üü° Good pipeline performance (2-10s total)")
    else:
        print_error("üî¥ Poor pipeline performance (> 10s total)")
    
    # Bottleneck analysis
    print_section("Bottleneck Analysis")
    if avg_agg_time > avg_update_time * 2:
        print_info("üîç Aggregation is the bottleneck (2x slower than update)")
    elif avg_update_time > avg_agg_time * 2:
        print_info("üîç Global update is the bottleneck (2x slower than aggregation)")
    else:
        print_success("‚öñÔ∏è Balanced performance between aggregation and update")
    
    # Scalability projection
    print_section("Scalability Projection")
    avg_clients = np.mean([d['num_clients'] for d in agg_timing])
    time_per_client = avg_agg_time / avg_clients if avg_clients > 0 else 0
    
    print(f"  Current: {avg_clients:.1f} clients in {avg_agg_time:.2f}s")
    print(f"  Time per client: {time_per_client:.3f}s")
    
    # Project for different client counts
    for clients in [10, 50, 100]:
        projected_time = time_per_client * clients
        print(f"  Projected {clients} clients: {projected_time:.2f}s")
        
        if projected_time < 5.0:
            print(f"    üü¢ Excellent scalability")
        elif projected_time < 20.0:
            print(f"    üü° Good scalability")
        else:
            print(f"    üî¥ Poor scalability")
    
    return {
        "aggregation_timing": agg_timing,
        "update_timing": update_timing,
        "pipeline_summary": {
            "avg_aggregation_time": avg_agg_time,
            "avg_update_time": avg_update_time,
            "avg_pipeline_time": avg_pipeline_time,
            "avg_clients": avg_clients,
            "time_per_client": time_per_client
        }
    }

def compare_with_baseline():
    """Compare timing with baseline (non-encrypted) performance"""
    print_header("Timing Comparison: Encrypted vs Non-Encrypted")
    
    print_section("Encryption Overhead Analysis")
    print("üîê Encrypted FL (Your System):")
    print("  ‚Ä¢ Client updates encrypted before transmission")
    print("  ‚Ä¢ Aggregation performed on encrypted data")
    print("  ‚Ä¢ Global model remains encrypted")
    print("  ‚Ä¢ Additional computational overhead for privacy")
    
    print("üîì Non-Encrypted FL (Baseline):")
    print("  ‚Ä¢ Client updates sent as plaintext")
    print("  ‚Ä¢ Aggregation performed on plaintext data")
    print("  ‚Ä¢ Global model stored as plaintext")
    print("  ‚Ä¢ Faster but no privacy protection")
    
    print_section("Expected Overhead")
    print("üìä Typical encryption overhead:")
    print("  ‚Ä¢ Aggregation: 2-5x slower than plaintext")
    print("  ‚Ä¢ Global Update: 1.5-3x slower than plaintext")
    print("  ‚Ä¢ Total Pipeline: 2-4x slower than plaintext")
    
    print_section("Privacy vs Performance Trade-off")
    print("üîê Privacy Benefits:")
    print("  ‚Ä¢ Data never leaves client devices in plaintext")
    print("  ‚Ä¢ Server never sees raw health data")
    print("  ‚Ä¢ GDPR/HIPAA compliance")
    print("  ‚Ä¢ Protection against data breaches")
    
    print("‚ö° Performance Cost:")
    print("  ‚Ä¢ 2-4x slower than non-encrypted FL")
    print("  ‚Ä¢ Higher computational requirements")
    print("  ‚Ä¢ More complex implementation")
    
    print_section("Recommendation")
    print("‚úÖ For Health Data: Encryption overhead is ACCEPTABLE")
    print("  ‚Ä¢ Privacy is more important than small performance loss")
    print("  ‚Ä¢ 2-4x overhead is reasonable for sensitive data")
    print("  ‚Ä¢ Your system achieves good balance")

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Federated Learning Timing Analysis")
    parser.add_argument("--explain", action="store_true", help="Explain timing importance")
    parser.add_argument("--aggregation", action="store_true", help="Analyze aggregation timing")
    parser.add_argument("--global-update", action="store_true", help="Analyze global update timing")
    parser.add_argument("--pipeline", action="store_true", help="Analyze complete pipeline timing")
    parser.add_argument("--compare", action="store_true", help="Compare with baseline performance")
    
    args = parser.parse_args()
    
    print("‚è±Ô∏è  Federated Learning Timing Analysis")
    print("=" * 70)
    
    # Explain timing importance if requested
    if args.explain:
        explain_timing_importance()
        return
    
    # Analyze aggregation timing
    if args.aggregation:
        analyze_aggregation_timing()
    
    # Analyze global update timing
    if args.global_update:
        analyze_global_update_timing()
    
    # Analyze complete pipeline timing
    if args.pipeline:
        analyze_pipeline_timing()
    
    # Compare with baseline
    if args.compare:
        compare_with_baseline()
    
    # Default: run all analyses
    if not any([args.aggregation, args.global_update, args.pipeline, args.compare]):
        print_info("Running complete timing analysis...")
        analyze_pipeline_timing()
        compare_with_baseline()
    
    # Summary
    print_header("Timing Analysis Summary")
    print("‚úÖ Timing analysis completed!")
    print("‚è±Ô∏è  Key metrics analyzed: Aggregation time, Update time, Pipeline time")
    print("üîê Encryption overhead assessed")
    print("üìà Scalability projections provided")
    
    if not any([args.aggregation, args.global_update, args.pipeline, args.compare, args.explain]):
        print_info("Use --help to see available options")
        print_info("Recommended: python analyze_timing.py --pipeline --compare")

if __name__ == "__main__":
    main()
